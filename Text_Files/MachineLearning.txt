Machine learning is all like find patterns in data and use those patterns to predict the future.
    Detecting credit card fraud
    Determining whether a customer is likely to switch to a competitor

We need to have some sample data and from that raw data, we'd need to identify some pattern and once the pattern is identified correctly, then we can use that pattern to predic the future.
We have some data containing patterns, now this data will be passed to machine learning algorithms which will generate the model after identifying the correct pattern. and now this model recognises those patterns in new data.

Machine learning process: we have lots of raw data (might be incomplete), So in order to get prepared data, we preprocess rawlogs with data preprocessing modules (kind of ingestion). Once data is prepared - apply learning algorithm to data using machine learning algorithms and it'll generate a model - Candidate model(not the final one) and we'll keep on applying ML algorithms to prepared data until we get the best model, Once we have the best model (or say chosen model), applications can use this model to supply new data to recognise patterns.

Training data - The prepared data used to create a model, Creating a model is called training a model
    Supervised learning: the value you want to predict is in the training data (most common)
    Unsupervised learning: The value you want to predict is not in training data

Creating training data(prepared data) is the most common task as raw data might have duplicates, missing values, corrupt data and can also contain some data which we know that it won't be of much useful while creating a model. (While working with US IBP, we know FR data won't be of much useful so we can omit FR data)

Categorising machine learning problems:
 1. Regression: We have data and we would like to find a line/curve that best fits the data
    Query: How many units of this product will we sell next month?

 2. Classification: We have data and we want to group this data into classes, when new data comes in - we want to know which class this new data belongs to.
    Query: Is this credit card fraudulent?

 3. Clustering: We have data and want to find out cluster in that data.
    Query: What are our customer segments?

(ASKING_THE_RIGHT_QUESTION) -> (PREPARING_DATA) -> (SELECTING_ALGORITHM) -> (TRAINING_THE_MODEL) -> (TESTING_THE_MODEL)

Predict if a person will develop diabetes:
    Understand the features in data
    Identify critical features
    Select data source

import numpy
import pandas
from matplotlib import pyplot

dataframe = pandas.read_csv("/path/to/csv/file")
dataframe.shape                                                     # will show the number of rows and columns in a tuple
dataframe.head(5)                                                   # will show top 5 records
dataframe.tail(5)                                                   # will show last 5 records
dataframe.get('some_column_name').head(5)                           # will work as a dictionary where some_column_name is column for dataframe type table

While creating a dataset, we may want to eliminate some columns which are not of used, which doesn't contain any value or which contains duplicates
Correlated columns contains same information in different format like ID and value associated with ID and doesn't add much information and may cause algorithm to get confused

dataframe.isnull().values.any()                                     # returns True if NULL values are present else will return False

We may need to delete the NULL value columns and correlated columns from the dataset and in order to check if dataframe contains any correlated column, we may plot a chart by which we'll get to know if any two columns are having correlation and once such type of columns are identified, we can delete them using command:
    del dataframe['some_column_name']

Now we have non-correlated data, null value freed data and we can mould this dataset according to our need (we may need to change data type or we may need to add some column), Say we want to change True to 1, False to 0 for some column.

dataframe.loc[dataframe['diabetes']==True]                          # will give all records where diabetes is True

Once the data is prepared, it's time to select an algorithm. We need to consider features -  Supervised/Non-Supervised, Some Value or True/False and it goes on, Once the initial algorithm is selected we can start training process.
Machine Learning Training: Letting specific data teach a machine learning algorithm to create a specific prediction. 

While creating a model or while training a model, We split prepared data into 2 parts - 70% data for training purpose and 30% for testing purpose.
Scikit-learn helps us to train the model and designed to work with numpy, scipy and pandas and is comfortable doing below mentioned tasks:
 1. Data splitting
 2. Pre processing
 3. Feature selection
 4. Model training
 5. Model tuning

Asking a right question would be like: Use the machine learning workflow to process and transform PIMA indian data to create a prediction model. This model must predict which people are likely to develop diabetes with 70% or greater accuracy.

Get the data and convert it into tidy datasets (easy to manipulate, model and visualize and have a specific structure).
    Each variable is column
    Each observation is a row
    Each type of observational unit is table
    

Style of Machine learning algorithms:
 1. Decision trees
 2. Neural networks
 3. Bayesian algorithms
 4. K-means algorithms

Ensemble algorithms are those algorithms which contain other algorithms and have multiple child algorithms to boost performance.

Candidate algorithms:
 1. Naive Bayes algorithms- based on likelihood and probability, every feature has the same weight, requires smaller amount of data
 2. Logistic Regression
 3. Decision tree - Binary tree and each comparision will lead to another path and goes on

Using a model: Application will call a model with values for features the model requires and model will return the predicted values using the featuresfsup

Core python libraries needed - 
- NUMPY Provides the foundation data structures and operations for scipy. These are arrays which are efficient to define and manipulate
- MATPLOTLIB Used for creating charts and plots
- PANDAS Provides data structures and functionality to quickly manipulate and analyze data. It is consisting of two things:
- SCIKIT-LEARN ML algorithms used for data analysis and data mining tasks
    Series: one dimensional array where rows and columns can be labelled
    Dataframes: multi dimensional array where rows and columns can be labelled
