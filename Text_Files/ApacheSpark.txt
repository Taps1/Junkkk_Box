$ spark-shell                               # for scala
$ pyspark                                     # for Python

scala> var a = 10
scala> a = 20
scala> a = "abc"                        # error
scala> var b = 20                       
scala> var b = 30                       # error, val is constant in scala
scala> val number = sc.textFile("a.txt")
scala> number.collect               # will return an array from memory

pip install pyspark
sudo pyspark
>>> sc.version
 number = sc.textFile("file:///home/talat/Talat/Projects/ApacheSpark/a.txt")
number.collect()                    # [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10']
export PYSPARK_DRIVER_PYTHON=ipython                                    # inside ~/.bashrc for interactive shell in Spark
fi_filter =  fi.filter(lambda line: "Talat" in line)                                            # will filter the data on basis of condition
fi.filter(lambda line: "Talat" in line).count()                                                # will give you count of filtered records
Project: Download the rawlogs in form of gzipped file from https://archive.org/download/stackexchange and upload them to HDFS.

Download apache & hadoop combined file from Apache.org
        tar -xzvf <tar_file>
        export SPARK_HOME="/home/ubuntu/spark-2.3.0-bin-hadoop2.7"
        export PATH=$SPARK_HOME:$PATH
        export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
        export PYSPARK_DRIVER_PYTHON="jupyter"
        export PYSPARK_DRIVER_PYTHON=OPTS="notebook"
        export PYSPARK_PYTHON=python3

        sudo chmod 777 spark-2.3.0-bin-hadoop2.7
        cd spark-2.3.0-bin-hadoop2.7/python
        python3
        cd spark-2.3.0-bin-hadoop2.7/
        sudo chmod 777 pyspark

Launch Jupyter notebook

        Once all are done, start project: analysing twitter APIs
        Create a twitter app, and get the tokes and authorized keys


