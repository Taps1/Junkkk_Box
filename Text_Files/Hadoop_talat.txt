{1}: Unit of Data:
————————————————
Bit, Byte, KB, MB, GB, TB, PB, EB, ZB, YB, XB

Java framework to store and process huge amount of data.
It can be used for batch processing but not for online processing.

Scalability:
	Horizontal - Add newer small machines.(Followed by Hadoop)
	Vertical - We’ll keep on improving the same machine as data grows.

{2}
Difference between hadoop and RDBMS:
Hadoop believe in horizontal scaling whereas SQL believes in vertical scaling.
Hadoop will be using map reduce programs instead of SQL
SQL is designed to store the data in tabular format (structured data), whereas Hadoop use key-value pair.

SQL is for real time processing data(Requires data immediately - Online transaction).
Hadoop is designed for batch processing.
In SQL, We can update the record, In Hadoop we won’t be able to.

Node: Any independent computer(processor) with its own RAM and HDD.
Cluster: A set of nodes in a network that can interact with each other.(Hadoop needs to be installed in all nodes).
Block: A small unit of data. 64MB is the block size in hadoop

Apache hadoop is free version, On top of it some companies has some customisations and known as Distributions,
Apache hadoop has 64MB block size, But distributions have 128MB block size.

So If I am having 200 MB file, Then it’ll be split into 4 blocks - 64MB, 64MB, 64MB, 8MB. Let’s have an example where We’ve 100 MB file.

Talat1	Parwez1	Software-Engineer1	Pilkhuwa1
Talat2	Parwez2	Software-Engineer2	Pilkhuwa2
Talat3	Parwez3	Software-Engineer3	Pilkhuwa3

Now when we talk that this file will split, Say this 100 MB file has been split in 2 blocks (64MB and 36MB).
	64MB
	Talat1	Parwez1	Software-Engineer1	Pilkhuwa1
	Talat2	Parwez2	Software-Engineer2

	36MB
						Pilkhuwa2
	Talat3	Parwez3	Software-Engineer3	Pilkhuwa3

There’s data corruption, 64 and 36 doesn’t contain atomic records but partial record(Pilkhuwa2), So hadoop will store the data in a way that record is complete on each block(know as Split).

	63.3MB
	Talat1	Parwez1	Software-Engineer1	Pilkhuwa1
	Talat2	Parwez2	Software-Engineer2	Pilkhuwa2

	36.7MB
	Talat3	Parwez3	Software-Engineer3	Pilkhuwa3

{3}
Hadoop Architecture - In cluster, there will be master-slave architecture where hadoop is installed on all systems.
Master will have NameNode process
	NameNode(Master)
	Secondary-NameNode
	JobTracker(Master)
	TaskTracker(Slave)
	DataNode(Slave)

Hadoop version should be same across the cluster nodes otherwise can create issue later on.
Hadoop’s own storage system: HDFS

Hadoop maintains replication factor which is by default: 3, So that no block of file is lost If any of the node goes down(Mean to say, It’s replicated across the nodes). If any datanode goes down then hadoop start searching for any suitable node to maintain the said replication factor. Say in meantime, the downed nodes came up - So there’ll be Overreplication then Hadoop will remove block from any node.

Speculative Execution: Suppose, Hadoop was executing B1 block somewhere in Node3, But Node3 is taking way much long time than configured one, So Hadoop will search all the nodes which have that block B1, Now Hadoop finds that Node2 has the block B1, So execution will start happening on Node2, and If Node2’s processing is way much faster than Node3, then result for block B1 from Node2 will be taken instead of taking from Node3.

When we submit our map reduce program to Hadoop, Hadoop will send that program to all the slave nodes. Namenode will be deciding where the block should go (Which block should gets executed on which datanode and maintains this register - .txt file has been divided into 4 parts, Now where all these blocks are residing across cluster.)


Download hadoop from link: https://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/

Inside Hadoop folder, We have 3 folders to mainly consider:
	bin & conf & lib

Modes in Hadoop:
	StandAlone hadoop - 1 Box, 5 JVMs
	Pseudo distributed mode hadoop
	Fully distributed mode hadoop - 3 Box, 5 JVMs

When we've multiple node cluster, then datanodes1, datanodes2 will be called as different machines having their own JVMs. 
If the cluster goes very huge, Then JobTracker will be kept in separate machine and this jobtracker will co-ordinate with all DataNodes and namenode will co-ordinate with JobTracker. 

Hadoop/conf:
	start-all.sh: 		starts all daemons one by one, Calls hadoop-config.sh, start-dfs.sh, start-mapred.sh
	start-dfs.sh:		Starts NameNode, DataNode, Secondary NameNode
	start-mapred.sh:	Starts JobTracker, TaskTracker

Whenever any command gets fired from terminal(say hadoop fs -put), then terminal(client) interacts with NameNode. Then NameNode will start talking with all datanodes available and status of all the datanodes (say their capacity to store the data) is acknowledged to client. Now It's the responsibility of client application to divide the data into chunks of 64 MB and after getting status of all datanodes from namenode, Client application will start putting data onto datanodes.

	step1: Client wants to put 200MB file and tell NameNode about it
	step2: NameNode talks to available datanodes and checks the status of them.
	step3: DataNodes acknowledged their status (like how much memory they're left with)
	step4: NameNode acknowledged to Client Application about the status of DataNodes
	step5: Client start sending data to different datanodes after breaking data into chunks.
	step6: Datanodes will start putting their data to another datanodes in order to maintain replication factor.
	step7: DataNodes will start communicating with NameNode and will tell about the blocks it has stored. Now NameNode will maintain 		       register.

The NameNode is a single point of failure for HDFS cluster, When the NameNode goes down, The file system goes Offline.

There's an optional secondary namenode that can be hosted on a separate machine, It only creates checkpoints of the namespace by merging the edit files into fsimage file and doesn't provide any real redundancy.

{7}
Now I want to submit a MapReduce program to hadoop cluster, So when I fire the command: "$ hadoop jar FindAllEmail.jar p1.ExtractEmail sample.txt /user/sumit/output", Then client application will talk to JobTracker, and JobTracker will contact to NameNode to check where the samplefile.txt file is stored and NameNode will tell in what all datanodes the blocks of file are present.

	step1: Client Application asks JobTracker to execute JAR command on Input file.
	step2: JobTracker asks for the location of blocks of file.
	step3: JobTracker got to know the location of all blocks and send the JAR file to all these tasktrackers(On DataNode). and will ask 		       TaskTrackers to execute corresponding blocks. (All execution instructions are given by JobTracker) and maintain register to 		       track which node is doing what. 
	step4: TaskTracker will keep on sending the amount of percentage of the work completed. Say one tasktracker went down, So JobTracker wil get to know a datanode is down If it doesn't receive any heartbeat for sometime, Now the block will be executed in some another datanode.

Hadoop execution sample command;
	hadoop jar MyWC.jar <HDFS_INPUT_PATH> <HDFS_OUTPUT_PATH>

When this job is submitted, JobTracker will take care of this and TaskTracker will execute it.

TaskTracker will call one more instance called as TaskInstance(by default - 2 map slots<only 1 block of file will get processed by each slot> and 2 reduce slots), In case of exception - TaskInstance will be terminated and TaskTracker won’t that’s why they are working on different JVM.

{8}
In map slot, The data will be converted into key-value pair.

Now this key-value paired data will be sorted (On basis of key), and after that “group phase” will be performed

from mapper:
apple 1
boy 1
cat 1 
apple 1
apple 1 

sort phase:
apple 1
apple 1
apple 1


group phase;
apple[1,1,1]
boy[1,1,1,1,1]

Reducer(key=fruit_name, value=array):

This reducer will be called for each key, and mapper will be called for each line.

{9}
Installation of Hadoop:
	$ sudo su
	$ addgroup hadoop
	$ adduser hduser				// dedicated hadoop user account` for running hadoop, (Not required but recommended) in order to separate Hadoop 							   installation from other applications and user accounts running on the same machine.
	# Need to add this newly created user(hduser) to newly created group(hadoop)
	$ sudo adduser hduser hadoop
	
	Add hduser to sudoers list so that hduser can perform admin tasks.
	$ sudo visudo 

	Add a line under ##ALLOW member of group sudo to execute any command anywhere in the format
	hduser ALL=(ALL)ALL

Hadoop requires SSH access to manage its nodes, i.e. remote machines plus local machine If I want to use Hadoop on it.
Generate the ssh key and add public key to authorized_keys so that whenever our datanode/namenode try to interact with each other and by adding these keys to authorized keys, Then they don’t need to verify it again and again.

	$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
	$ chmod 700 ~/.ssh/authorized_keys

If ssh is not running, then start it by using: 
	$ sudo /etc/init.d/ssh start

But before this, One might have to install ssh(client) and sshd(server):
	$ sudo apt-get remove openssh-client openssh-server
	$ sudo apt-get install openssh-client openssh-server

Hadoop and IPV6 do not agree on the meaning of 0.0.0.0 address, thus it’s advisable to disable IPV6 adding the following lines @the end of /etc/sysctl.conf:
	$ vim /etc/sysctl.conf
	# disable ipv6
	net.ipv6.conf.all.disable_ipv6 = 1	
	net.ipv6.conf.default.disable_ipv6 = 1	
	net.ipv6.conf.lo.disable_ipv6 = 1

	•Check if IPv6 is disabled.
	After a system reboot the output of 
	hduser@ubuntu:~$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6 
	should be 1, meaning that IPV6 is actually disabled

Download the hadoop and put it inside /usr/local (unzip and untag it), After that We can create a symlink with “hadoop” name in order to retrieve it by “hadoop” name.
	sudo ln -s hadoop-1.2.1 hadoop
	Give permissions to the hadoop-1.2.1 folder after altering chown as well(hduser:hadoop)

Add Java location to Hadoop so that It can recognise Java
	 $ vim /usr/local/hadoop/conf/hadoop-env.sh
	 export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true 
	 export HADOOP_HOME_WARN_SUPPRESS="TRUE" 
	 export JAVA_HOME=/usr/local/java/jdk1.8.0_31 

	$ vim ~/.bashrc  

	# Set Hadoop-related environment variables 
	export HADOOP_HOME=/usr/local/hadoop  
	# Set JAVA_HOME (we will also configure JAVA_HOME directly for Hadoop later on) 
	export JAVA_HOME=/usr/local/java/jdk1.8.0_31  
	# Some convenient aliases and functions for running Hadoop-related commands 

	unaliasfs&> /dev/null 
	aliasfs="hadoop fs" 
	unaliashls&> /dev/null 
	aliashls="fs -ls"

Make sure to add HADOOP_HOME and JAVA_HOME path to PATH variable, otherwise I’ll keep on getting HADOOP command not found

————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————
Inside file “hdfs-site.xml”, replication factor is set, which is configurable:
		<property>
			<name>dfs.replication</name>
			<value>3</value>
		</property>


Update core-site.xml file
	Add the following snippets between the <configuration> ... </configuration> tags in/usr/local/hadoop/conf/core-site.xml:
		vim  /usr/local/hadoop/conf/core-site.xml  
		<!-- In: conf/core-site.xml --> 

		<property> 
			<name>hadoop.tmp.dir</name> 
			<value>/app/hadoop/tmp</value> 
			<description>A base for other temporary directories.</description> 	
		</property>  
		 <property>
			 <name>fs.default.name</name> 				
			<value>hdfs://localhost:54310</value> 
			<description>The name of the default file system.A URI whose scheme and authority determine the FileSystem implementation.The uri's scheme determines 					the config property. The uri's authority is used to determine the host, port, etc. for a filesystem.
			</description> 
		</property>


Update mapred-site.xml file
	Add the following to /usr/local/hadoop/conf/mapred-site.xml  between<configuration> ... </configuration>
		vim /usr/local/hadoop/conf/mapred-site.xml  
		<property> 
			<name>mapred.job.tracker</name>
			<value>localhost:54311</value> 
			<description>The host and port that the MapReduce job tracker runs at. If "local", then jobs are run in-process as a single map and reduce task.
		 	</description> 
		</property>  


Update hdfs-site.xml file
	Add the following to /usr/local/hadoop/conf/hdfs-site.xml  between<configuration> ... </configuration>

	vim /usr/local/hadoop/conf/hdfs-site.xml  

	<property>
	   <name>dfs.replication</name>
	   <value>1</value>
	   <description>Default block replication.The actual number of replications can be specified when the file is created.The default is used if replication is not 	   specified in create time.</description>
	</property>
————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————

Whenever a new installation is done, Its recommended to format the name node:
	$ hadoop namenode -format

Single node cluster is ready to use and can be verified via: 
	$ start-all.sh
	$ jps

DFS runs on port: 54310
MapReduce runs on port: 54311


{10}{1}
Hadoop commands:
	Hadoop hdfs command:
		hadoop fs  <command>
	hadoop job command:
		hadoop jar <command>


	hadoop fs -mkdir <foldername> — will create folder inside /users/<user_name>/
	hadoop fs -mkdir /<foldername> — will create folder @root
	
	list out all folders recursively:
		hadoop fs -lrs <foldername>
	
	Copy file from local to HDFS:
		hadoop fs -put <file_name_on_local> <location_on_hdfs> OR
		hadoop fs -copyFromLocal <file_name_on_local> <location_on_hdfs>

Say, I have .tar.gz file and I want to copy only one file out of it to HDFS for processing.
	$ tar xvzf data.tar.gz/fruits.txt -O | hadoop fs -put - myData1/fruits.txt.

I can even write content to the file(Not with existing one) in HDFS:
	$ hadoop fs -put - myfile.txt

I can see the content of file in terminal as well:
	$ hadoop fs -get <file_name> -
	$ hadoop fs -copyToLocal <file_name> -

put VS copyFromLocal and get VS copToLocal
	copyFromLocal will only deal with local file system, While put deals with any file system (In case of writing to a file from one HDFS to another HDFS, put will be used/from stdin).

Setting replication factor of a specific file:
	$ hadoop fs -setrep 3 <file_name>

NameNode takes care of all ls/mkdir operations.

{10}{2}
If I want to write content from stdin to any of the HDFS file, then I’ll be saying:
	$ hadoop fs -put - <file_name>

I can also change the group of file present in HDFS (chmod & chown can also be done as well)
	$ hadoop fs -chgrp -R <group_name> <file_name/directory_name>

cp command is used to copy files from one HDFS location to another HDFS location
	$ hadoop fs -cp <source_hdfs_location> <destination_hdfs_location>

If I want to get the data from HDFS to local then It’s good to merge the whole data into a single file and then get it. For this purpose getmerge is used.
	$ hadoop fs -getmerge <file_name1> <file_name2> <output_filename>

Instead of copying a file to HDFS from local, We can move the file to HDFS:
	$ hadoop fs -moveFromLocal <file_name_on_local> <destination_folder>
f
We can also move files from one HDFS location to another HDFS location.
	$ hadoop fs -mv <source_location> <destination_location>

In order to clear recycle bin in HDFS, Use expunge(thrashing concept)
	$ hadoop fs -expunge

The hadoop test is used for file operations,
	$ hadoop fs -test [ezd] <file_path> followed by echo $?
		e - existence of file
		z - file size is zero or not
		d - If the path is directory or not

We can create an empty file too in Hadoop: (Can’t be written)
	$ hadoop fs -touchz <file_name>

To count the number of files in a directory:
	$ hadoop fs -count <directory_name>

When we start writing a hadoop map/reduce program with eclipse - chose: new/other/java-projet, As we've chosed java_project, then we'll be adding some JAR files.

Some jar files, I'll be getting from $HADOOP_HOME and some files, I'll be getting from $HADOOP_HOME/lib.

Standard eclipse version may not have map-reduce plugin, We may need to add this plugin. To verify check in eclipse - windows/open_perspective/other/map-reduce
Once map-reduce plugin is added to location, We may add a new DFS location to it and Can directly perform Upload/Delete/Mkdir operations from Eclipse

Create a project in Eclipse - create another new folder inside it named as lib, copy hadoop-core-1.2.1.jar inside lib folder. Once all required JAR files are present under lib location, then we've to add this lib folder while building a path, add these JARS

In mapper-reducer code, 3 resources are required
	Mapper class
	Reducer class
	Main method class called as driver program

As of Now, all these methods are written as separate class/file and for driver(holds main(String args[])) some configuration objects are needed.
	1. Create a configuration object
		Configuration conf = new Configuration();        // hadoop related files only (^+shift+O)

	2. Create a Job class which'll take care of MapReduce job (JobTracker)
		Job job = new Job(conf, "MyWordCountJob")

	3. link your driver class with the job
		job.setJarByClass(MyDriver.class);               // Coz MyDriver contains main() method

In mapper, One has to emit the data which’ll be received by Reducer, So mapper has to emit the data in Context and reducer will take that data from Context

To call a non-static method, We need an object
Each input file is considered as block and each block is executed by One task instance.
	word1.txt - 100MB — 2 blocks — B1, B2
	word2.txt - 200MB — 2 blocks — B1, B2, B3, B4
	word3.txt - 5KB — 2 blocks — B1

So, here we’ve 7 blocks and each block is executed by One Task Instance, So totally 7 times, map() method will be called.

<SAFE MODE IN HADOOP>:
	When we trigger start-all.sh, All daemons starts one by one, So when NN starts - It enters into safe mode (All datanodes will be discovered and checked how many are live, How many are dead and all the information about the files present in DN will be given to NN) and No write operation will happen during this phase and Once this checking and all is done-NN will leave SafeMode

There's a chance due to some problem, NN is not able to come out from SafeMode. To manually make NN come out of SafeMode:
	$ hadoop dfsadmin -safemode leave
	$ hdfs dfsadmin -safemode leave

In hadoop interface, We've two things:
	Mapper Class
	Mapper Interface

extends - Class(New API)
implements - Interface(Older API)

If we havn't configured any mapper and reducer class, then default classes will be called which are:
Old API:
	Mapper: Mapper class in API
	Reducer: Reducer class

New API
	Mapper: IdentityMapper class in old API
	Reducer: IdentityReducer class in old API

<MAVEN>:
	Lots of different eclipse projects may have dependency, So Maven is a Project management and dependency management tool. Like classes in project-1 might be required in project-2, So Maven plays a role. Maven maintains a central repository from where JAR files can be downloaded. You need to install maven as well - sudo apt-get install maven3

Once the maven is installed, It actually creates ".m2" folder inside /home/hduser which we can remove as well	
Create a new "maven-project" in eclipse - There it'll ask some questions for group-Id, artifact-Id which are taken as:
	hadoop-core-1.2.1.jar
	company_name:	Apache -- 	Company_Name
	version_number:	1.2.1 --  	version
	jar_name:	hadoop-core -- 	ArtifactId

Once I create the Maven project, .m2 will be created again in ~/.m2 folder and ~/.m2/repository will contain JARs. Now the project which we just created as Maven project (eclipse), We can create classes as MyMapper.java, MyReducer.java, MyDriver.java. Now these *.class files might be requiring some mandatory JARs in order to execute Hadoop wordcount example - So What we can do is: Create a lib folder, Copy all required files into that and add it to the class path ... Or we can use the functionality of Maven to add JAR files into POM.xml file.
	- Open POM.xml file --> dependencies --> Add dependencies

In case, Unable to add via "Add" button, Just copy the <dependency></dependency> tag from mvnrepository.com.
And run it as normal java application and it can simply be converted to a JAR file by exporting to java-jar-next-finish. The Maven project can be executed in various option like: run as: Maven Install (which will generate a jar file in target folder <specified in pom.xml file>), This jar file can be executed as MapReduce program by 
	hadoop jar <jar_file_name> <package_name>.<main_class>
	hadoop jar WC.jar p1.MyDriver (# when we run by this command, MapReduce program runs on pseudo distributed mode, and When executed directly from Eclipse - It runs on local mode)

Working with Pseudo distributed mode in Hadoop:
What all the programs I have done are performed on Local mode
	Create a Jar file
	Submit the JAR to JobTracker

Syntax to submit Map-Reduce jar to JobTracker is:
	$ hadoop jar MyWCJAR.jar p1.WordCountDriver args…
(Once, This command is executed, MapReduce won’t be executing on Local mode but in Pseudo distributed mode) and all the SOPs won’t get printed on the screen, What If I still wanted to see all of my SOPs

To view sysout (SOPs) data in distributed mode, Go to JobTracker URL:
	http://localhost:50030/jobtracker.jsp
	http://localhost:50070/dfshealth.jsp

Where MyWCJAR is jar file name
p1 is package name

WordCountDriver is driver’s file name

Once I am into job tracker URL, Go to job_id —> mapper —> map —> Task logs —> syslog (logs generated by default)/ stdoutlog (logs printed by Programmer)

If we need to supply the arguments to the program, Don't run it as "java_application", But run it as: "Run Configurations" and set the variables in Program Arguments (Under Arguments section) separated with Space.

If you want to do samething using maven,here is the way.

Open terminal and navigate to pom.xml location

	>mvn clean
	>mvn install

In case your main class doesnot expect any argument use the below method from the place where pom.xml is present

   	>mvn exec:java -Dexec.mainClass="p1.MyDriver"  

In case your main class expects some argument then you use the below approach
	> mvn exec:java -Dexec.mainClass="p1.MyDriver" -Dexec.args="input_data output_data "

NameNode and Secondary NameNode:
	When we format the namenode for the first time by: $ hadoop namenode -format, A directory got created which is: {dfs.name.dir} <searchable in below mentioned 3 files>. Which can be found under the .xmls files of hadoop configuration

The hadoop configuration files which we modified is:
	core-site.xml
	mapred-site.xml
	hdfs-site.xml

In hadoop-core-1.2.1 extracted jar file, these above listed 3 files (default ones) are present, Check for {dfs.name.dir} which will go for {hadoop.tmp.dir} <configured in core-site.xml> -- This is going to lead us to location: /app/hadoop/tmp/dfs where secondary_namenode folder is present, So whenever We format the namenode, by default namenode related directories are created under `/app/hadoop/tmp/dfs/name` folder
